{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = torch.arange(0,20)\n",
    "params = torch.randn(3).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37646.48828125\n",
      "8068.16943359375\n",
      "2471.02001953125\n",
      "1411.8424072265625\n",
      "1211.3868408203125\n",
      "1173.42822265625\n",
      "1166.21923828125\n",
      "1164.828369140625\n",
      "1164.538818359375\n",
      "1164.457763671875\n"
     ]
    }
   ],
   "source": [
    "def apply_step(params,prn=True):\n",
    "    \n",
    "    speed = time*3 + (time-9.5)**2 + 1\n",
    "    \n",
    "    a,b,c = params \n",
    "    pred = a*(time**2) + b*time + c\n",
    "    loss = ((pred - speed)**2).mean()\n",
    "    loss.backward()\n",
    "    lr = 1e-5\n",
    "    params.grad\n",
    "    params.data -= lr * params.grad.data\n",
    "    params.grad=None\n",
    "    if prn: print(loss.item())\n",
    "    return pred\n",
    "for i in range(10): apply_step(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0296)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "from torch.nn import functional\n",
    "import numpy\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "\n",
    "\n",
    "\n",
    "threes = (path/'train/3').ls().sorted()\n",
    "sevens = (path/'train/7').ls().sorted()\n",
    "valid_3s = (path/'valid/3').ls().sorted()\n",
    "valid_7s = (path/'valid/7').ls().sorted()\n",
    "\n",
    "stacked_3s = [tensor(Image.open(o)) for o in threes]\n",
    "stacked_7s = [tensor(Image.open(o)) for o in sevens]\n",
    "stacked_valid3s = [tensor(Image.open(o)) for o in valid_3s]\n",
    "stacked_valid7s = [tensor(Image.open(o)) for o in valid_7s]\n",
    "stacked_3s = torch.stack(stacked_3s).float()/255\n",
    "stacked_7s = torch.stack(stacked_7s).float()/255\n",
    "stacked_valid_3s = torch.stack(stacked_valid3s).float()/255\n",
    "stacked_valid_7s = torch.stack(stacked_valid7s).float()/255\n",
    "average_3s = (stacked_3s).mean(0)\n",
    "average_7s = (stacked_7s).mean(0)\n",
    "average_valid_3s = (stacked_valid_3s).mean(0)\n",
    "average_valid_7s = (stacked_valid_7s).mean(0)\n",
    "\n",
    "def L1_loss(average,real):\n",
    "    result = (average - real).abs().mean()\n",
    "    return result\n",
    "\n",
    "def mean_sq_error_loss(average,real):    \n",
    "    result = ((average-real)**2).sqrt().mean()\n",
    "    return result\n",
    "\n",
    "L1_loss(average_3s,stacked_3s[0])\n",
    "L1_loss(average_7s,stacked_7s[0])\n",
    "L1_loss(average_valid_3s,stacked_valid_3s[0])\n",
    "L1_loss(average_valid_7s,stacked_valid_7s[0])\n",
    "\n",
    "mean_sq_error_loss(average_3s,stacked_3s[0])\n",
    "mean_sq_error_loss(average_7s,stacked_7s[0])\n",
    "mean_sq_error_loss(average_valid_3s,stacked_valid_3s[0])\n",
    "mean_sq_error_loss(average_valid_7s,stacked_valid_7s[0])\n",
    "\n",
    "F.l1_loss(average_3s,stacked_3s[0])\n",
    "F.l1_loss(average_7s,stacked_7s[0])\n",
    "F.l1_loss(average_valid_3s,stacked_valid_3s[0])\n",
    "F.l1_loss(average_valid_7s,stacked_valid_7s[0])\n",
    "\n",
    "F.mse_loss(average_3s,stacked_3s[0])\n",
    "F.mse_loss(average_7s,stacked_7s[0])\n",
    "F.mse_loss(average_valid_3s,stacked_valid_3s[0])\n",
    "F.mse_loss(average_valid_7s,stacked_valid_7s[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.cat([stacked_3s,stacked_7s]).view(-1,28*28)\n",
    "train_y = tensor([0]*len(threes) + [1]*len(sevens)).unsqueeze(1)\n",
    "\n",
    "dset = list(zip(train_x,train_y))\n",
    "x,y=dset[0]\n",
    "valid_x = torch.cat([stacked_valid_3s,stacked_valid_7s]).view(-1,28*28)\n",
    "valid_y = tensor([0]*len(valid_3s) + [1]*len(valid_7s)).unsqueeze(1)\n",
    "valid_dset = list(zip(valid_x,valid_y))\n",
    "\n",
    "# initialize paramters\n",
    " \n",
    "def init_params(size,std=1.0): \n",
    "    return (torch.randn(size)*std).requires_grad_() \n",
    "\n",
    "weights = init_params(28*28,1)\n",
    "bias = init_params(1)\n",
    "\n",
    "def linear1(xb):\n",
    "    return xb@weights + bias\n",
    "\n",
    "preds = linear1(train_x)\n",
    "corrects = (preds>0.0).float() == train_y\n",
    "corrects.float().mean().item()\n",
    "weights[0] *= 1.0001\n",
    "preds = linear1(train_x)\n",
    "((preds>0.0).float() == train_y.float().mean().item())\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def mnist_loss(predictions,targets):\n",
    "    predictions=predictions.sigmoid()\n",
    "    return torch.where(targets==1,1-predictions,predictions).mean()\n",
    "\n",
    "loss = mnist_loss(preds,train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([ 9, 20, 13, 17, 25,  7]), ('j', 'u', 'n', 'r', 'z', 'h')),\n",
       " (tensor([22,  8, 11, 19,  4, 14]), ('w', 'i', 'l', 't', 'e', 'o')),\n",
       " (tensor([ 6,  1, 24, 23,  0, 12]), ('g', 'b', 'y', 'x', 'a', 'm')),\n",
       " (tensor([ 2, 10, 16,  3, 18, 21]), ('c', 'k', 'q', 'd', 's', 'v')),\n",
       " (tensor([15,  5]), ('p', 'f'))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll = range(15)\n",
    "dl = DataLoader(coll,batch_size=5,shuffle=True)\n",
    "list(dl)\n",
    "\n",
    "ds = L(enumerate(string.ascii_lowercase))\n",
    "dl = DataLoader(ds,batch_size=6,shuffle=True)\n",
    "list(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params(28*28,1)\n",
    "bias = init_params(1)\n",
    "dl = DataLoader(dset,batch_size = 256)\n",
    "xb,yb = first(dl)\n",
    "valid_dl = DataLoader(valid_dset,batch_size=256)\n",
    "batch = train_x[:4]\n",
    "preds = linear1(batch)\n",
    "loss = mnist_loss(preds,train_y[:4])\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb,yb,model):\n",
    "    preds=model(xb)\n",
    "    loss = mnist_loss(preds,yb)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0090), tensor([0.0629]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.grad.zero_()\n",
    "bias.grad.zero_()\n",
    "calc_grad(batch,train_y[:4],linear1)\n",
    "weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,lr,params):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb,yb,model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()\n",
    "            \n",
    "def batch_accuracy(xb,yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>.5)==yb\n",
    "    return correct.float().mean()\n",
    "\n",
    "\n",
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb),yb) for xb,yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(),4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8412 0.8637 0.8859 0.8966 0.9077 0.9131 0.9185 0.9218 0.9252 0.9281 0.931 0.9319 0.9339 0.9348 0.9358 0.9363 0.9368 0.9373 0.9382 0.9397 "
     ]
    }
   ],
   "source": [
    "batch_accuracy(linear1(batch),train_y[:4])\n",
    "lr = .1\n",
    "params = weights,bias\n",
    "train_epoch(linear1,lr,params)\n",
    "validate_epoch(linear1)\n",
    "\n",
    "for i in range(20):\n",
    "    train_epoch(linear1,lr,params)\n",
    "    print(validate_epoch(linear1), end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(28*28,1)\n",
    "w,b = linear_model.parameters()\n",
    "\n",
    "class BasicOptim:\n",
    "    \n",
    "    def __init__(self,params,lr): \n",
    "        self.params,self.lr = list(params),lr\n",
    "        \n",
    "    def step(self,*args,**kwargs):\n",
    "        for p in self.params:\n",
    "            p.data-=p.grad.data *self.lr\n",
    "    \n",
    "    def zero_grad(self,*args,**kwargs):\n",
    "        for p in self.params:\n",
    "            p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5439 0.8647 0.9336 0.9526 0.9634 0.9638 0.9658 0.9663 0.9678 0.9692 0.9707 0.9717 0.9717 0.9721 0.9726 0.9726 0.9726 0.9726 0.9731 0.9731 "
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.196875</td>\n",
       "      <td>0.365177</td>\n",
       "      <td>0.562316</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.115353</td>\n",
       "      <td>0.179271</td>\n",
       "      <td>0.876840</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.081332</td>\n",
       "      <td>0.107115</td>\n",
       "      <td>0.934249</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.064630</td>\n",
       "      <td>0.080330</td>\n",
       "      <td>0.954367</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.055403</td>\n",
       "      <td>0.067099</td>\n",
       "      <td>0.962218</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.049734</td>\n",
       "      <td>0.059282</td>\n",
       "      <td>0.962709</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.045899</td>\n",
       "      <td>0.054109</td>\n",
       "      <td>0.965653</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.043090</td>\n",
       "      <td>0.050414</td>\n",
       "      <td>0.967125</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.040904</td>\n",
       "      <td>0.047627</td>\n",
       "      <td>0.968106</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.039129</td>\n",
       "      <td>0.045438</td>\n",
       "      <td>0.970069</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr=.1\n",
    "opt = BasicOptim(linear_model.parameters(),lr)\n",
    "def train_epoch(model):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb,yb,model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "validate_epoch(linear_model)\n",
    "\n",
    "def train_model(model,epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(validate_epoch(model),end=' ')\n",
    "        \n",
    "train_model(linear_model,20)\n",
    "\n",
    "dl = DataLoaders(dl,valid_dl)\n",
    "learn = Learner(dl,nn.Linear(28*28,1),opt_func=SGD,\n",
    "               loss_func=mnist_loss, metrics=batch_accuracy)\n",
    "\n",
    "learn.fit(10,lr=lr)\n",
    "\n",
    "def simple_net(xb):\n",
    "    res = xb@w1 + b1\n",
    "    res = res.max(tensor(0.0))\n",
    "    res = res@w2 + b2\n",
    "    return res\n",
    "\n",
    "w1 = init_params((28*28,30))\n",
    "b1 = init_params(30)\n",
    "w2 = init_params((30,1))\n",
    "b2 = init_params(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
